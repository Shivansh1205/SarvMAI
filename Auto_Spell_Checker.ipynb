{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvqOW4Gv6B3jmlCUAaVdVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivansh1205/SarvMAI/blob/main/Auto_Spell_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, random, time, csv\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# ---------- normalization & edit distance ----------\n",
        "def normalize(word: str) -> str:\n",
        "    w = word.lower().strip()\n",
        "    w = re.sub(r'(a){2,}', 'a', w)\n",
        "    w = re.sub(r'(i){2,}', 'i', w)\n",
        "    w = re.sub(r'(e){2,}', 'e', w)\n",
        "    w = re.sub(r'(o){2,}', 'o', w)\n",
        "    w = re.sub(r'(u){2,}', 'u', w)\n",
        "    w = w.replace('w','v').replace('ph','f').replace('bh','b')\n",
        "    w = re.sub(r'[^a-z0-9]', '', w)\n",
        "    return w\n",
        "\n",
        "def levenshtein(a: str, b: str) -> int:\n",
        "    if a == b: return 0\n",
        "    if len(a) < len(b): a, b = b, a\n",
        "    prev = list(range(len(b)+1))\n",
        "    for i, ca in enumerate(a,1):\n",
        "        curr = [i]\n",
        "        for j, cb in enumerate(b,1):\n",
        "            insert = curr[j-1]+1\n",
        "            delete = prev[j]+1\n",
        "            replace = prev[j-1] + (0 if ca==cb else 1)\n",
        "            curr.append(min(insert, delete, replace))\n",
        "        prev = curr\n",
        "    return prev[-1]\n",
        "\n",
        "# ---------- BK-tree ----------\n",
        "class BKNode:\n",
        "    def __init__(self, word):\n",
        "        self.word = word\n",
        "        self.children = {}\n",
        "\n",
        "class BKTree:\n",
        "    def __init__(self, dist_fn=levenshtein):\n",
        "        self.root = None\n",
        "        self.dist = dist_fn\n",
        "\n",
        "    def add(self, word):\n",
        "        if self.root is None:\n",
        "            self.root = BKNode(word)\n",
        "            return\n",
        "        node = self.root\n",
        "        while True:\n",
        "            d = self.dist(word, node.word)\n",
        "            if d in node.children:\n",
        "                node = node.children[d]\n",
        "            else:\n",
        "                node.children[d] = BKNode(word)\n",
        "                break\n",
        "\n",
        "    def query(self, word, max_distance):\n",
        "        if self.root is None: return []\n",
        "        res = []\n",
        "        stack = [self.root]\n",
        "        while stack:\n",
        "            node = stack.pop()\n",
        "            d = self.dist(word, node.word)\n",
        "            if d <= max_distance:\n",
        "                res.append((node.word, d))\n",
        "            low, high = d - max_distance, d + max_distance\n",
        "            for cd, child in node.children.items():\n",
        "                if low <= cd <= high:\n",
        "                    stack.append(child)\n",
        "        return res\n",
        "\n",
        "# ---------- k-gram fallback ----------\n",
        "def build_kgram_index(words, k=3):\n",
        "    idx = defaultdict(set)\n",
        "    for w in words:\n",
        "        norm = normalize(w)\n",
        "        grams = set([norm[i:i+k] for i in range(len(norm)-k+1)]) if len(norm) >= k else {norm}\n",
        "        for g in grams:\n",
        "            idx[g].add(w)\n",
        "    return idx\n",
        "\n",
        "def top_k_by_kgram_overlap(query, kgram_index, k=3, top_n=20):\n",
        "    norm_q = normalize(query)\n",
        "    grams = set([norm_q[i:i+k] for i in range(len(norm_q)-k+1)]) if len(norm_q) >= k else {norm_q}\n",
        "    counter = Counter()\n",
        "    for g in grams:\n",
        "        for w in kgram_index.get(g, []):\n",
        "            counter[w] += 1\n",
        "    return [w for w,_ in counter.most_common(top_n)]\n",
        "\n",
        "# ---------- build structures ----------\n",
        "with open(\"reference.txt\", encoding=\"utf-8\") as f:\n",
        "    reference = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# norm_map: normalized -> canonical\n",
        "norm_map = {}\n",
        "for w in reference:\n",
        "    norm_map[normalize(w)] = w\n",
        "\n",
        "bk = BKTree()\n",
        "for norm_w in set(normalize(w) for w in reference):\n",
        "    bk.add(norm_w)\n",
        "kgram_index = build_kgram_index(reference, k=3)\n",
        "\n",
        "def match_best(input_word, bk_tree, kgram_index, norm_map, bk_radius=2):\n",
        "    norm_in = normalize(input_word)\n",
        "    # BK-tree search\n",
        "    bk_res = bk_tree.query(norm_in, max_distance=bk_radius)\n",
        "    candidates = []\n",
        "    if bk_res:\n",
        "        candidates.extend(bk_res)\n",
        "    else:\n",
        "        # fallback via k-gram overlap\n",
        "        ks = top_k_by_kgram_overlap(input_word, kgram_index, k=3, top_n=20)\n",
        "        for w in ks:\n",
        "            norm_w = normalize(w)\n",
        "            d = levenshtein(norm_in, norm_w)\n",
        "            candidates.append((norm_w, d))\n",
        "    if not candidates:\n",
        "        # brute force\n",
        "        best = None; min_d = None\n",
        "        for norm_ref in norm_map:\n",
        "            d = levenshtein(norm_in, norm_ref)\n",
        "            if min_d is None or d < min_d:\n",
        "                min_d = d; best = norm_ref\n",
        "        return norm_map.get(best,best), min_d\n",
        "    candidates.sort(key=lambda x: x[1])\n",
        "    best_norm, best_dist = candidates[0]\n",
        "    return norm_map.get(best_norm, best_norm), best_dist\n",
        "\n",
        "# ---------- synthetic test set ----------\n",
        "random.seed(42)\n",
        "def stretch_vowels(word):\n",
        "    return \"\".join(c*random.choice([1,2]) if c.lower() in \"aeiou\" else c for c in word)\n",
        "def typo_delete(word):\n",
        "    if len(word) <= 1: return word\n",
        "    i = random.randrange(len(word))\n",
        "    return word[:i] + word[i+1:]\n",
        "def typo_swap(word):\n",
        "    if len(word) < 2: return word\n",
        "    i = random.randrange(len(word)-1)\n",
        "    lst = list(word); lst[i], lst[i+1] = lst[i+1], lst[i]\n",
        "    return \"\".join(lst)\n",
        "def case_variant(word):\n",
        "    return word.lower() if word.isupper() else word.upper()\n",
        "\n",
        "test_pairs = []\n",
        "for w in random.sample(reference, 30):\n",
        "    variants = set([\n",
        "        stretch_vowels(w),\n",
        "        typo_swap(w),\n",
        "        typo_delete(w),\n",
        "        case_variant(w),\n",
        "        typo_swap(stretch_vowels(w))\n",
        "    ])\n",
        "    for v in variants:\n",
        "        test_pairs.append((v, w))\n",
        "\n",
        "# ---------- evaluation ----------\n",
        "total = len(test_pairs)\n",
        "exact = 0\n",
        "top3 = 0\n",
        "edit_sum = 0.0\n",
        "mismatches = []\n",
        "\n",
        "start = time.perf_counter()\n",
        "for err, gold in test_pairs:\n",
        "    best, dist = match_best(err, bk, kgram_index, norm_map, bk_radius=2)\n",
        "    # get top3 for recall\n",
        "    # gather candidates similarly\n",
        "    candidates = []\n",
        "    bk_res = bk.query(normalize(err), max_distance=2)\n",
        "    if bk_res:\n",
        "        candidates.extend(bk_res)\n",
        "    else:\n",
        "        ks = top_k_by_kgram_overlap(err, kgram_index, k=3, top_n=20)\n",
        "        for w in ks:\n",
        "            norm_w = normalize(w)\n",
        "            d = levenshtein(normalize(err), norm_w)\n",
        "            candidates.append((norm_w, d))\n",
        "    if not candidates:\n",
        "        for nr in norm_map:\n",
        "            candidates.append((nr, levenshtein(normalize(err), nr)))\n",
        "    candidates.sort(key=lambda x: x[1])\n",
        "    top3_list = [norm_map.get(n, n) for n,_ in candidates[:3]]\n",
        "\n",
        "    if best == gold:\n",
        "        exact += 1\n",
        "    if gold in top3_list:\n",
        "        top3 += 1\n",
        "    edit_sum += levenshtein(best.lower(), gold.lower()) / max(len(best), len(gold), 1)\n",
        "    if best != gold:\n",
        "        mismatches.append((err, gold, top3_list))\n",
        "end = time.perf_counter()\n",
        "\n",
        "exact_acc = exact / total\n",
        "top3_recall = top3 / total\n",
        "avg_edit = edit_sum / total\n",
        "time_taken = end - start\n",
        "per_word = time_taken / total if total else 0\n",
        "\n",
        "print(\"=== Evaluation ===\")\n",
        "print(f\"Total cases: {total}\")\n",
        "print(f\"Exact-match accuracy: {exact}/{total} = {exact_acc:.2%}\")\n",
        "print(f\"Top-3 recall: {top3}/{total} = {top3_recall:.2%}\")\n",
        "print(f\"Avg normalized edit distance: {avg_edit:.3f}\")\n",
        "print(f\"Total time: {time_taken:.4f}s, per-word: {per_word*1000:.2f}ms\")\n",
        "print(\"Sample mismatches:\", mismatches[:5])\n",
        "\n",
        "# save proof\n",
        "with open(\"gold_bktest.tsv\",\"w\",encoding=\"utf-8\",newline=\"\") as f:\n",
        "    w=csv.writer(f,delimiter=\"\\t\")\n",
        "    w.writerow([\"error\",\"gold\"])\n",
        "    for err,gold in test_pairs:\n",
        "        w.writerow([err,gold])\n",
        "with open(\"predicted_bktest.tsv\",\"w\",encoding=\"utf-8\",newline=\"\") as f:\n",
        "    w=csv.writer(f,delimiter=\"\\t\")\n",
        "    w.writerow([\"error\",\"predicted\"])\n",
        "    for err,gold in test_pairs:\n",
        "        best,_=match_best(err,bk,kgram_index,norm_map,bk_radius=2)\n",
        "        w.writerow([err,best])\n",
        "print(\"Saved gold and predicted for proof.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kWCkUm_866L",
        "outputId": "c7d025e8-c729-415c-843d-b5779b359d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation ===\n",
            "Total cases: 147\n",
            "Exact-match accuracy: 136/147 = 92.52%\n",
            "Top-3 recall: 147/147 = 100.00%\n",
            "Avg normalized edit distance: 0.032\n",
            "Total time: 0.1160s, per-word: 0.79ms\n",
            "Sample mismatches: [('Shail', 'Sahil', ['Sunil', 'Sahil']), ('aKviitaa', 'Kavita', ['Savita', 'Kavita']), ('Rkeha', 'Rekha', ['Sneha', 'Rekha', 'Neha']), ('Kaal', 'Kajal', ['Ram', 'Kajal', 'Aam']), ('rPitii', 'Priti', ['Ritu', 'Priti'])]\n",
            "Saved gold and predicted for proof.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEkxVh-7KrBX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}